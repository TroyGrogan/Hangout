///////////////////////////////////////////////////////////////////////////////////////////////////
TO DOWNLOAD THE AI MODEL, GO INTO THE DJANGO BACKEND TERMINAL, 
AND RUN THIS COMMAND:

Installing this from the main home directory of the app 
(aka the folder that contains: 'backend', 'env', 'frontend', README.md, etc.):


cd backend/ai_model && curl -L -o "tinyllama-1.1b-chat-v1.0.Q8_0.gguf" "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q8_0.gguf?download=true"


^^This command works because we cd into the backend/ai_model directory. ^^
Explanation of the command:

curl: The command-line tool itself.

-L: 
This option tells curl to follow redirects.
Hugging Face links often use redirects, so this is important.

-o "backend/ai_model/tinyllama-1.1b-chat-v1.0.Q8_0.gguf": 
This specifies the output file path and name.

"backend/ai_model/": 
This is the directory where you want to save the file.

"tinyllama-1.1b-chat-v1.0.Q8_0.gguf": 
This is the name of the file as it will be saved on your system.

"https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q8_0.gguf?download=true": 
This is the direct download URL for the model file.

Before running:
- Ensure curl is installed: Most macOS and Linux systems have it by default. 
  If not, you might need to install it.

- Ensure the directory exists: Make sure the backend/ai_model/ directory 
  exists in your project. If it doesn't, 
  create it first (mkdir -p backend/ai_model).

- Run from the correct location: Execute this command from the root directory 
  of your project (Capstone-Computing-Project---Hangout-Web-App/) 
  so that the relative path backend/ai_model/ is correct.

- The download will take some time as the model file is about 1.1GB. 
  You'll see a progress indicator in your terminal. Once it's complete, the model file will be in the specified directory.



FOR RENDER IN PARTICULAR:
Steps to Access Render Shell:
1. Upgrade to a paid tier (Shell access is not available on free tier)
2. Go to your backend service in Render dashboard
3. Click the "Shell" button in the top navigation
4. Run the command: 
cd ai_model && curl -L -o "tinyllama-1.1b-chat-v1.0.Q8_0.gguf" "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q8_0.gguf?download=true"

Important Notes:
‚ö†Ô∏è File Persistence: 
Files downloaded this way will persist across deployments on Render's paid tiers 
(unlike some other platforms)

‚ö†Ô∏è Download Size: The model is ~1.1GB, so it will take several minutes to download

‚ö†Ô∏è Disk Space: Make sure your Render plan has enough storage for the model file

You're spot on with the path - 
since Render's shell starts you in the backend directory, 
cd ai_model is exactly right! üöÄ


SO, IN SHORT, RUN THIS COMMAND THROUGH YOUR SHELL IN YOUR BACKEND DEPLOYED ENVIRONMENT,
THROUGH THE SHELL:

cd ai_model && curl -L -o "tinyllama-1.1b-chat-v1.0.Q8_0.gguf" "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q8_0.gguf?download=true"
