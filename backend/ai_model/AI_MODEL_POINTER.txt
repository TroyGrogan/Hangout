///////////////////////////////////////////////////////////////////////////////////////////////////
TO DOWNLOAD THE AI MODEL, GO INTO THE DJANGO BACKEND TERMINAL, 
AND RUN THIS COMMAND:

Installing this from the main home directory of the app 
(aka the folder that contains: 'backend', 'env', 'frontend', README.md, etc.):


cd backend/ai_model && curl -L -o "tinyllama-1.1b-chat-v1.0.Q8_0.gguf" "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q8_0.gguf?download=true"


^^This command works because we cd into the backend/ai_model directory. ^^
Explanation of the command:

curl: The command-line tool itself.

-L: 
This option tells curl to follow redirects.
Hugging Face links often use redirects, so this is important.

-o "backend/ai_model/tinyllama-1.1b-chat-v1.0.Q8_0.gguf": 
This specifies the output file path and name.

"backend/ai_model/": 
This is the directory where you want to save the file.

"tinyllama-1.1b-chat-v1.0.Q8_0.gguf": 
This is the name of the file as it will be saved on your system.

"https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q8_0.gguf?download=true": 
This is the direct download URL for the model file.

Before running:
- Ensure curl is installed: Most macOS and Linux systems have it by default. 
  If not, you might need to install it.

- Ensure the directory exists: Make sure the backend/ai_model/ directory 
  exists in your project. If it doesn't, 
  create it first (mkdir -p backend/ai_model).

- Run from the correct location: Execute this command from the root directory 
  of your project (Capstone-Computing-Project---Hangout-Web-App/) 
  so that the relative path backend/ai_model/ is correct.

- The download will take some time as the model file is about 1.1GB. 
  You'll see a progress indicator in your terminal. Once it's complete, the model file will be in the specified directory.



FOR RENDER IN PARTICULAR:
Steps to Access Render Shell:
1. Upgrade to a paid tier (Shell access is not available on free tier)
2. Go to your backend service in Render dashboard
3. Click the "Shell" button in the top navigation
4. Run the command: 
cd ai_model && curl -L -o "tinyllama-1.1b-chat-v1.0.Q8_0.gguf" "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q8_0.gguf?download=true"

Important Notes:
âš ï¸ File Persistence: 
Files downloaded this way will persist across deployments on Render's paid tiers 
(unlike some other platforms)

âš ï¸ Download Size: The model is ~1.1GB, so it will take several minutes to download

âš ï¸ Disk Space: Make sure your Render plan has enough storage for the model file

You're spot on with the path - 
since Render's shell starts you in the backend directory, 
cd ai_model is exactly right! ðŸš€


SO, IN SHORT, RUN THIS COMMAND THROUGH YOUR SHELL IN YOUR BACKEND DEPLOYED ENVIRONMENT,
THROUGH THE SHELL:

cd ai_model && curl -L -o "tinyllama-1.1b-chat-v1.0.Q8_0.gguf" "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q8_0.gguf?download=true"


Make sure that this is the start command in render so that timeout issues don't occurr:

gunicorn --workers 1 --worker-class sync --timeout 1200 --keep-alive 1200 --graceful-timeout 1200 --max-requests 1000 --max-requests-jitter 50 --preload backend.wsgi:application --bind 0.0.0.0:$PORT

What This Does:
--timeout 1200 = 20 minutes worker timeout
--keep-alive 1200 = 20 minutes keep-alive timeout
--graceful-timeout 1200 = 20 minutes graceful shutdown timeout
--worker-class sync = Synchronous workers (better for long-running tasks)
--preload = Preload the application (can help with memory management)
--bind 0.0.0.0:$PORT = Bind to Render's required port

Steps:
Click "Edit" next to the Start Command
Replace the existing command with the one above
Save/Apply the changes
Render will automatically redeploy with the new timeout settings

This will bypass the Procfile entirely and force Render 
to use our extended timeout settings directly! ðŸš€

Once you update this, the AI model should have the full 20 minutes 
to complete its processing without timing out.




NEW MODEL AFTER FINDING OUT SYSTEM SPECS FOR REAL.

->

My Shell: 
CPU:
render@srv-d0v0k3fdiees73ccb5bg-594cd9fd5-dcdwm:~/project/src/backend$ lscpu
Architecture:             x86_64
  CPU op-mode(s):         32-bit, 64-bit
  Address sizes:          48 bits physical, 48 bits virtual
  Byte Order:             Little Endian
CPU(s):                   16
  On-line CPU(s) list:    0-15
Vendor ID:                AuthenticAMD
  Model name:             AMD EPYC 7R13 Processor
    CPU family:           25
    Model:                1
    Thread(s) per core:   2
    Core(s) per socket:   8
    Socket(s):            1
    Stepping:             1
    BogoMIPS:             5300.00
    Flags:                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb 
                          rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 ss
                          e4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext 
                          ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero 
                          xsaveerptr rdpru wbnoinvd arat npt nrip_save vaes vpclmulqdq rdpid
Virtualization features:  
  Hypervisor vendor:      KVM
  Virtualization type:    full
Caches (sum of all):      
  L1d:                    256 KiB (8 instances)
  L1i:                    256 KiB (8 instances)
  L2:                     4 MiB (8 instances)
  L3:                     32 MiB (1 instance)
NUMA:                     
  NUMA node(s):           1
  NUMA node0 CPU(s):      0-15
Vulnerabilities:          
  Gather data sampling:   Not affected
  Itlb multihit:          Not affected
  L1tf:                   Not affected
  Mds:                    Not affected
  Meltdown:               Not affected
  Mmio stale data:        Not affected
  Reg file data sampling: Not affected
  Retbleed:               Not affected
  Spec rstack overflow:   Vulnerable: Safe RET, no microcode
  Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl
  Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization
  Spectre v2:             Mitigation; Retpolines; IBPB conditional; IBRS_FW; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected
  Srbds:                  Not affected
  Tsx async abort:        Not affected
render@srv-d0v0k3fdiees73ccb5bg-594cd9fd5-dcdwm:~/project/src/backend$ 

RAM:
render@srv-d0v0k3fdiees73ccb5bg-594cd9fd5-dcdwm:~/project/src/backend$ free -h
               total        used        free      shared  buff/cache   available
Mem:            61Gi        22Gi       2.6Gi       136Mi        37Gi        38Gi
Swap:             0B          0B          0B

render@srv-d0v0k3fdiees73ccb5bg-594cd9fd5-dcdwm:~/project/src/backend$ 

JUST THE OPENCHAT 3.6 QUANTIZED MODEL:
https://huggingface.co/bartowski/openchat-3.6-8b-20240522-GGUF

DOWNLOAD LINK:
https://huggingface.co/bartowski/openchat-3.6-8b-20240522-GGUF/resolve/main/openchat-3.6-8b-20240522-Q4_K_M.gguf?download=true


downloading the Q4_k_m MODEL THROUGH COMMAND LINE:

cd backend/ai_model && curl -L -o "tinyllama-1.1b-chat-v1.0.Q8_0.gguf" "https://huggingface.co/bartowski/openchat-3.6-8b-20240522-GGUF/resolve/main/openchat-3.6-8b-20240522-Q4_K_M.gguf?download=true"